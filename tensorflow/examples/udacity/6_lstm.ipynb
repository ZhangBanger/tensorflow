{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urllib.urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print 'Found and verified', filename\n",
    "  else:\n",
    "    print statinfo.st_size\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return f.read(name)\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print \"Data size\", len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print train_size, train_text[:64]\n",
    "print valid_size, valid_text[:64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0 Unexpected character: ï\n",
      "0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print 'Unexpected character:', char\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print char2id('a'), char2id('z'), char2id(' '), char2id('ï')\n",
    "print id2char(1), id2char(26), id2char(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size / batch_size\n",
    "    self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in xrange(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in xrange(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(valid_batches.next())\n",
    "print batches2string(valid_batches.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in xrange(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in xrange(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 3.29625964165 learning rate: 10.0\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "v ejdfmygub thflsonyquvxcaqfdhsk ogix nuoerfvgbw mbforlhifekhey hodtkirusxg ekas\n",
      "iavilh tp twpnmzuaze a  kkbwnn ouhoetyv e  a guweiernamkl ope jsijccahoxncn  sdj\n",
      "hpzeuhtfxsdv   fdwt njfpqasshhngmgyzqhe  qwgnhcrtobn lnqyl  hddgjrqjleehtqomt ri\n",
      "t xal s o y qstenzytc gl kttykmnsezoqsneb of akieq avq est upa  uziktfbrna mo yn\n",
      "vymntiu usypakmee lz ahose ie g js vk dithuv vhaoqwmaehhtufqv jdqd beoau lqltypa\n",
      "================================================================================\n",
      "Validation set perplexity: 20.09\n",
      "Average loss at step 100 : 2.59334707737 learning rate: 10.0\n",
      "Minibatch perplexity: 10.11\n",
      "Validation set perplexity: 10.94\n",
      "Average loss at step 200 : 2.25841280937 learning rate: 10.0\n",
      "Minibatch perplexity: 8.67\n",
      "Validation set perplexity: 9.19\n",
      "Average loss at step 300 : 2.11425981402 learning rate: 10.0\n",
      "Minibatch perplexity: 8.36\n",
      "Validation set perplexity: 8.20\n",
      "Average loss at step 400 : 2.01309205055 learning rate: 10.0\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 500 : 1.96492365003 learning rate: 10.0\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 600 : 1.91668482542 learning rate: 10.0\n",
      "Minibatch perplexity: 7.17\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 700 : 1.87601760626 learning rate: 10.0\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 800 : 1.84889372706 learning rate: 10.0\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 900 : 1.82307735562 learning rate: 10.0\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1000 : 1.82941587567 learning rate: 10.0\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "tahanlies zerofor sse astor resceved to veal their hat a pie mar prions in fouri\n",
      "gerned essibly aid wht to kiact of ceurnse tate fatth as of five five helipona g\n",
      "greawing dip in do sembr of ling hradar wus she if puric as quam dric to the per\n",
      "ment in d abliets who in over other cell and corimange hid stalterik sta his wel\n",
      "p foture espixec or bille of jiy venlel or d revuge pussich is a simem by the an\n",
      "================================================================================\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 1100 : 1.8081052804 learning rate: 10.0\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1200 : 1.79937368989 learning rate: 10.0\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1300 : 1.75802039862 learning rate: 10.0\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1400 : 1.75816878915 learning rate: 10.0\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1500 : 1.75232472658 learning rate: 10.0\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1600 : 1.72072138309 learning rate: 10.0\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1700 : 1.71735693216 learning rate: 10.0\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1800 : 1.69367052317 learning rate: 10.0\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 1900 : 1.69756188989 learning rate: 10.0\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2000 : 1.69420565248 learning rate: 10.0\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "ard also replation brananstifria rubstinite and wrankud britipes firtues yeamey \n",
      "kban ver wash englivialed becansic sumnci s work caft one face bust overal a one\n",
      "forotismes plas of had assephy ranjo manmales be constrenced also be labide sch \n",
      "x intercaide he grevesson uson pyt teran pops of the ofber dny the symans these \n",
      "ght concharly termall workn is a conjemside internac three quectasi sups what ne\n",
      "================================================================================\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2100 : 1.68836155772 learning rate: 10.0\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2200 : 1.67696436405 learning rate: 10.0\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2300 : 1.65089736342 learning rate: 10.0\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2400 : 1.65447855473 learning rate: 10.0\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2500 : 1.63265138626 learning rate: 10.0\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2600 : 1.6317091012 learning rate: 10.0\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2700 : 1.61694609761 learning rate: 10.0\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2800 : 1.6376709044 learning rate: 10.0\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2900 : 1.66943553567 learning rate: 10.0\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3000 : 1.64833713055 learning rate: 10.0\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "al interage been for thira squest ks eight four seven one one festly five eveny \n",
      "n states bold onlywalss i was that s by who vists the control when u a bazes ame\n",
      "ed of anta literal comes brothment the been hesmisting the first declistor with \n",
      "viloniesits higheroblesson regaless os a an one eight hister one to an one nine \n",
      "p the was wde lawshqulers used and surent spendautation offer offectived impucs \n",
      "================================================================================\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3100 : 1.67173395753 learning rate: 10.0\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3200 : 1.64115639806 learning rate: 10.0\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3300 : 1.63923685908 learning rate: 10.0\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3400 : 1.63291699648 learning rate: 10.0\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3500 : 1.62799531937 learning rate: 10.0\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3600 : 1.62247418284 learning rate: 10.0\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3700 : 1.63863226891 learning rate: 10.0\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3800 : 1.64533452153 learning rate: 10.0\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3900 : 1.60279293418 learning rate: 10.0\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4000 : 1.59258216262 learning rate: 10.0\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "z the prexarured track two zero one nine largine tradents of phyiloge dictionary\n",
      "zard boundnerics cal a planety was a foujh informic and such somen ridjer one ni\n",
      "ance that zero control arain ooke trames villaped propussion rather panget compa\n",
      "velstrer smarlial has major pop regular disonpo a seab of desoly b and the one n\n",
      "pholwa resspricare dotazal of contromany invoeth ltwor mat in the willar five fo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4100 : 1.62072578907 learning rate: 10.0\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4200 : 1.63083323121 learning rate: 10.0\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4300 : 1.64891809583 learning rate: 10.0\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4400 : 1.63742528915 learning rate: 10.0\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4500 : 1.60309667587 learning rate: 10.0\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4600 : 1.61180724263 learning rate: 10.0\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4700 : 1.60961055517 learning rate: 10.0\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4800 : 1.61880202651 learning rate: 10.0\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4900 : 1.60564808369 learning rate: 10.0\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5000 : 1.60487278819 learning rate: 1.0\n",
      "Minibatch perplexity: 5.18\n",
      "================================================================================\n",
      "ban domemit decoverates the used tenrs the in ougle dewife general it mangurary \n",
      "mind any appoy s s field searal hoil he god aner k one one johnmena eurr caped i\n",
      "z classus a tomachifern laprian bovin five eight zero zero five guery tomication\n",
      "h producent the acro regard approsighane was by salewning chazoper action report\n",
      "warg posses with from ataig thn is was thered coce hered matning give four eight\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5100 : 1.56438746929 learning rate: 1.0\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5200 : 1.57738445401 learning rate: 1.0\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300 : 1.56592685938 learning rate: 1.0\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5400 : 1.57560129642 learning rate: 1.0\n",
      "Minibatch perplexity: 4.16\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5500 : 1.60391899824 learning rate: 1.0\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5600 : 1.60667678833 learning rate: 1.0\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5700 : 1.60763002038 learning rate: 1.0\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5800 : 1.60523679614 learning rate: 1.0\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5900 : 1.58979549646 learning rate: 1.0\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6000 : 1.60934155703 learning rate: 1.0\n",
      "Minibatch perplexity: 4.52\n",
      "================================================================================\n",
      "x the strupt it is usl after hungery three extervation forces poire seven males \n",
      "nousemina a advesses eight most chnines a stons side to modern disarding the per\n",
      " known the surg shortert a liby hab he system beloastemf up anicks a te hustross\n",
      "y suring her the contimunal was barkvies araung ite shed all only aim i resided \n",
      "pselbies ca that to the sides simple from younces rockes whot trife wherse midi \n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100 : 1.61166888714 learning rate: 1.0\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6200 : 1.60347838044 learning rate: 1.0\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6300 : 1.61644495964 learning rate: 1.0\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6400 : 1.57689108729 learning rate: 1.0\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6500 : 1.58079015613 learning rate: 1.0\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6600 : 1.58062589288 learning rate: 1.0\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6700 : 1.58262122512 learning rate: 1.0\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6800 : 1.61560004354 learning rate: 1.0\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6900 : 1.59839581013 learning rate: 1.0\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 7000 : 1.60060753226 learning rate: 1.0\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "batleham is madks sounce into the oppaning a sticling of the one nine one two da\n",
      "re divelsififed the supportanes or male us razor to hame he non through operatio\n",
      "merza cis taksels the americe mearbally embrostence are is was rate uniple new b\n",
      "tere and genre one demanded by the morplating also aho whey problets in the dete\n",
      "zers sinfo as the glony semeive himpostime is rose befope the lage are layemics \n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in xrange(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colabVersion": "0.3.2",
  "colab_default_view": {},
  "colab_views": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
